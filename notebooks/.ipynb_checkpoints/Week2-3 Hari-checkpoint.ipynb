{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "416ec47d-7a4c-45d1-87a5-8601c6ef9669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and download NLTK resources\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources (only needed once)\n",
    "nltk.download('punkt')       # Standard tokenizer resource\n",
    "nltk.download('punkt_tab')   # Additional resource required by some tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Scikit-learn imports for vectorization, modeling, and evaluation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea912fed-45e5-4233-8f0c-98799b991fdb",
   "metadata": {},
   "source": [
    "Section C: Data Loading and Preprocessing (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "386f05e1-2662-4c1b-9473-f6a3b301cb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame:\n",
      "Index(['post_id', 'post_title', 'comment_id', 'author', 'comment_text',\n",
      "       'score', 'created_utc', 'is_submitter', 'contains_noon_mention', 'type',\n",
      "       'Sentiment', 'Positive', 'Negative', 'Neutral', 'Overall',\n",
      "       'Sentiment_Nltk', 'Target Value', 'Unnamed: 17'],\n",
      "      dtype='object')\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "   post_id                                         post_title comment_id  \\\n",
      "0  1ec7r4l  My bank just woke me up at 2:00 in the morning...    ley1hzs   \n",
      "1  1ec7r4l  My bank just woke me up at 2:00 in the morning...    lf0s1re   \n",
      "2  1ec7r4l  My bank just woke me up at 2:00 in the morning...    lf06mu0   \n",
      "3  1ec7r4l  My bank just woke me up at 2:00 in the morning...    lf1m0tu   \n",
      "4  1ec7r4l  My bank just woke me up at 2:00 in the morning...    lfartnt   \n",
      "\n",
      "                author                                       comment_text  \\\n",
      "0          hanihaneefa  Noon doesn't ask for otp when using cards... H...   \n",
      "1       SnooGuavas4756  UPDATE: Noon.com has reached out to me on Redd...   \n",
      "2            Dansdan84  The bank should be able to give the money back...   \n",
      "3              haruror  This happened to me recently. Why is it always...   \n",
      "4  Impressive_End_1222  Something similar happened and in case of no O...   \n",
      "\n",
      "   score       created_utc  is_submitter  contains_noon_mention     type  \\\n",
      "0     75   26/07/2024 2:29         False                   True  comment   \n",
      "1      9  26/07/2024 16:05          True                   True  comment   \n",
      "2      2  26/07/2024 12:37         False                   True  comment   \n",
      "3      2  26/07/2024 19:11         False                   True  comment   \n",
      "4      2   28/07/2024 9:47         False                   True  comment   \n",
      "\n",
      "  Sentiment  Positive  Negative  Neutral  Overall Sentiment_Nltk  \\\n",
      "0   Neutral     0.000     0.000    1.000   0.0000        Neutral   \n",
      "1  Positive     0.166     0.054    0.780   0.5574       Positive   \n",
      "2  Negative     0.000     0.104    0.896  -0.2960       Negative   \n",
      "3  Negative     0.000     0.141    0.859  -0.2023       Negative   \n",
      "4  Positive     0.123     0.113    0.764   0.1027       Positive   \n",
      "\n",
      "   Target Value Unnamed: 17  \n",
      "0          True         NaN  \n",
      "1          True         NaN  \n",
      "2          True         NaN  \n",
      "3          True         NaN  \n",
      "4          True         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Define the path to your CSV file (update the path as needed)\n",
    "csv_file_path = \"../Data/processed/noon_filtered_comments_with_sentiment_nltk.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {csv_file_path}. Please check the path.\")\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Error loading CSV file: {e}\")\n",
    "\n",
    "# Remove any extra whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Print out the column names for inspection\n",
    "print(\"Columns in the DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Verify that required columns exist: 'comment_text' and 'Target Value'\n",
    "if 'comment_text' not in df.columns or 'Target Value' not in df.columns:\n",
    "    raise KeyError(\"The CSV file must contain both 'comment_text' and 'Target Value' columns.\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nFirst few rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Preprocess the \"Target Value\" column to boolean\n",
    "df['Target Value'] = df['Target Value'].apply(lambda x: True if str(x).strip().upper() == \"TRUE\" else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5056e-11c5-4ca3-9cd7-639b57aed147",
   "metadata": {},
   "source": [
    "Text Analytics Pipeline â€“ Preprocessing & Feature Extraction (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bf563-9d06-4d49-8d40-0375ea47c6ab",
   "metadata": {},
   "source": [
    "Define a Custom Tokenizer (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f54fe159-841a-4bb3-a237-543e871f32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords and the Porter Stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenize text by:\n",
    "      - Converting to lowercase\n",
    "      - Removing punctuation\n",
    "      - Tokenizing using NLTK's word_tokenize\n",
    "      - Removing English stopwords\n",
    "      - Applying stemming using PorterStemmer\n",
    "    Returns:\n",
    "      A list of processed tokens.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc4123-6ae8-424d-979f-c3c61b464a6d",
   "metadata": {},
   "source": [
    "Prepare Features and Split Data (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e89b6830-b9ab-4f4c-9c93-8d46ffc0b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features (X) and labels (y)\n",
    "X = df['comment_text']\n",
    "y = df['Target Value']\n",
    "\n",
    "# Split the data into training and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a26bcd-8a14-4489-87d2-6c68e1d428d0",
   "metadata": {},
   "source": [
    "Build the Pipeline and Hyperparameter Tuning (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a707df3-f8e5-41b6-8de0-dcf597b0b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best cross-validation score: 0.94\n",
      "Best parameters:  {'clf__C': 0.001, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress the specific warning about token_pattern not being used\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The parameter 'token_pattern' will not be used since 'tokenizer' is not None\"\n",
    ")\n",
    "\n",
    "# Build the text analytics pipeline with TFâ€“IDF and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        tokenizer=custom_tokenizer,   # Use our custom tokenizer\n",
    "        ngram_range=(1, 2),           # Experiment with unigrams and bigrams (you can try (1,1) or (1,3))\n",
    "        min_df=5,                     # Ignore terms that appear in fewer than 5 documents\n",
    "        stop_words=None               # Stopwords are already removed in our custom tokenizer\n",
    "    )),\n",
    "    ('clf', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "# Define the grid of parameters for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155718a-9ef1-4a50-b022-f717f4e677ea",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10b06598-8e2a-45a6-8d55-cf7f5654ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00         5\n",
      "        True       0.95      1.00      0.98        99\n",
      "\n",
      "    accuracy                           0.95       104\n",
      "   macro avg       0.48      0.50      0.49       104\n",
      "weighted avg       0.91      0.95      0.93       104\n",
      "\n",
      "Test Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set using the best found model\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Print a detailed classification report and accuracy\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2dc326-3dd7-4c99-a2a6-0028d5110637",
   "metadata": {},
   "source": [
    "Visualization and Insights (Markdown & Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22137524-72f8-4c44-a663-af4acca9cb86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Generate a word cloud for positive comments\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# (Make sure to install the wordcloud package: pip install wordcloud)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Filter out positive comments from the original DataFrame (or use a different condition)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Example: Generate a word cloud for positive comments\n",
    "# (Make sure to install the wordcloud package: pip install wordcloud)\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out positive comments from the original DataFrame (or use a different condition)\n",
    "positive_comments = df[df['Sentiment_Nltk'] == \"Positive\"]['comment_text']\n",
    "\n",
    "# Combine all positive comments into one large string\n",
    "positive_text = \" \".join(positive_comments.astype(str).tolist())\n",
    "\n",
    "# Create a word cloud object and generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Positive Comments\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63dadb-a5a9-4d93-95f5-b2b55ef374c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
