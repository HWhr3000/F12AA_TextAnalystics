{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386f05e1-2662-4c1b-9473-f6a3b301cb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hariharakumarrathinar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data packages (only needs to be done once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn imports for vectorization, classification and evaluation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54fe159-841a-4bb3-a237-543e871f32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the stemmer and lemmatizer (choose one based on your preference)\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def custom_tokenizer(text, use_stemming=True, use_lemmatization=False):\n",
    "    \"\"\"\n",
    "    Process text by lowercasing, removing punctuation, tokenizing,\n",
    "    removing stopwords and applying stemming or lemmatization.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "        use_stemming (bool): If True, apply stemming.\n",
    "        use_lemmatization (bool): If True, apply lemmatization.\n",
    "                                  (If both are True, stemming is applied.)\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of processed tokens.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using regex (this removes punctuation characters)\n",
    "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
    "    \n",
    "    # Tokenize the text using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Apply stemming or lemmatization (if desired)\n",
    "    if use_stemming:\n",
    "        tokens = [ps.stem(t) for t in tokens]\n",
    "    elif use_lemmatization:\n",
    "        tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e7a84d-dfbf-4e1e-91b2-55c2fdabf340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV file must contain both 'comment_text' and 'Sentiment' columns.\n",
      "   post_id                                     post_title comment_id  \\\n",
      "0  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mjkdv   \n",
      "1  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mgv03   \n",
      "2  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mfqls   \n",
      "3  1i9vydl                   Warranty from Noon or Amazon    m9lj27y   \n",
      "4  1i9vydl                   Warranty from Noon or Amazon    m9lhuq1   \n",
      "\n",
      "             author                                       comment_text  score  \\\n",
      "0    99DragonMaster                          will reach border by noon      2   \n",
      "1   AgileBadger5988  Be careful of blowing dust during late morning...      9   \n",
      "2     jamesdongdong  Apart from boring things, you can managed. How...      1   \n",
      "3  Agitated-Fox2818  You are looking at a tag in noon app saying 1 ...      2   \n",
      "4             m2bop  Are you sure? There's nothing indicating that ...      1   \n",
      "\n",
      "           created_utc  is_submitter  contains_noon_mention     type  \\\n",
      "0  2025-01-28 16:33:14          True                   True  comment   \n",
      "1  2025-01-28 16:12:52         False                   True  comment   \n",
      "2  2025-01-28 16:03:56         False                   True  comment   \n",
      "3  2025-01-28 10:38:59         False                   True  comment   \n",
      "4  2025-01-28 10:28:09          True                   True  comment   \n",
      "\n",
      "          subreddit  \n",
      "0  DubaiPetrolHeads  \n",
      "1  DubaiPetrolHeads  \n",
      "2  DubaiPetrolHeads  \n",
      "3               NaN  \n",
      "4               NaN  \n"
     ]
    }
   ],
   "source": [
    "# Define the path to your CSV file (update the path as needed)\n",
    "csv_file_path = \"../Data/processed/Final_Noon_Datasets.csv\"\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Check for the expected columns\n",
    "if 'comment_text' not in df.columns or 'Sentiment' not in df.columns:\n",
    "    print(\"The CSV file must contain both 'comment_text' and 'Sentiment' columns.\")\n",
    "    exit()\n",
    "\n",
    "# Optionally, display a few rows to verify the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07aade-06fd-4f6b-8475-b1c4e3635de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
