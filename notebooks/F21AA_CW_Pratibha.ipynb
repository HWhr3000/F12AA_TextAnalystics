{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Noon Dataset for Vader Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>contains_noon_mention</th>\n",
       "      <th>type</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mjkdv</td>\n",
       "      <td>99DragonMaster</td>\n",
       "      <td>will reach border by noon</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-28 16:33:14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mgv03</td>\n",
       "      <td>AgileBadger5988</td>\n",
       "      <td>Be careful of blowing dust during late morning...</td>\n",
       "      <td>9</td>\n",
       "      <td>2025-01-28 16:12:52</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mfqls</td>\n",
       "      <td>jamesdongdong</td>\n",
       "      <td>Apart from boring things, you can managed. How...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-28 16:03:56</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1i9vydl</td>\n",
       "      <td>Warranty from Noon or Amazon</td>\n",
       "      <td>m9lj27y</td>\n",
       "      <td>Agitated-Fox2818</td>\n",
       "      <td>You are looking at a tag in noon app saying 1 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-28 10:38:59</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1i9vydl</td>\n",
       "      <td>Warranty from Noon or Amazon</td>\n",
       "      <td>m9lhuq1</td>\n",
       "      <td>m2bop</td>\n",
       "      <td>Are you sure? There's nothing indicating that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-28 10:28:09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                     post_title comment_id  \\\n",
       "0  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mjkdv   \n",
       "1  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mgv03   \n",
       "2  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mfqls   \n",
       "3  1i9vydl                   Warranty from Noon or Amazon    m9lj27y   \n",
       "4  1i9vydl                   Warranty from Noon or Amazon    m9lhuq1   \n",
       "\n",
       "             author                                       comment_text  score  \\\n",
       "0    99DragonMaster                          will reach border by noon      2   \n",
       "1   AgileBadger5988  Be careful of blowing dust during late morning...      9   \n",
       "2     jamesdongdong  Apart from boring things, you can managed. How...      1   \n",
       "3  Agitated-Fox2818  You are looking at a tag in noon app saying 1 ...      2   \n",
       "4             m2bop  Are you sure? There's nothing indicating that ...      1   \n",
       "\n",
       "           created_utc  is_submitter  contains_noon_mention     type  \\\n",
       "0  2025-01-28 16:33:14          True                   True  comment   \n",
       "1  2025-01-28 16:12:52         False                   True  comment   \n",
       "2  2025-01-28 16:03:56         False                   True  comment   \n",
       "3  2025-01-28 10:38:59         False                   True  comment   \n",
       "4  2025-01-28 10:28:09          True                   True  comment   \n",
       "\n",
       "          subreddit  \n",
       "0  DubaiPetrolHeads  \n",
       "1  DubaiPetrolHeads  \n",
       "2  DubaiPetrolHeads  \n",
       "3               NaN  \n",
       "4               NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "commentList_df = pd.read_csv(\"./Final_Noon_Datasets.csv\", header=0)\n",
    "commentList_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3598 entries, 0 to 3597\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   post_id                3598 non-null   object\n",
      " 1   post_title             3598 non-null   object\n",
      " 2   comment_id             3598 non-null   object\n",
      " 3   author                 3360 non-null   object\n",
      " 4   comment_text           3598 non-null   object\n",
      " 5   score                  3598 non-null   int64 \n",
      " 6   created_utc            3598 non-null   object\n",
      " 7   is_submitter           3598 non-null   bool  \n",
      " 8   contains_noon_mention  3598 non-null   bool  \n",
      " 9   type                   3598 non-null   object\n",
      " 10  subreddit              3079 non-null   object\n",
      "dtypes: bool(2), int64(1), object(8)\n",
      "memory usage: 260.1+ KB\n"
     ]
    }
   ],
   "source": [
    "commentList_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>contains_noon_mention</th>\n",
       "      <th>type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>vader_label</th>\n",
       "      <th>vader_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mjkdv</td>\n",
       "      <td>99DragonMaster</td>\n",
       "      <td>will reach border by noon</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-28 16:33:14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mgv03</td>\n",
       "      <td>AgileBadger5988</td>\n",
       "      <td>Be careful of blowing dust during late morning...</td>\n",
       "      <td>9</td>\n",
       "      <td>2025-01-28 16:12:52</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mfqls</td>\n",
       "      <td>jamesdongdong</td>\n",
       "      <td>Apart from boring things, you can managed. How...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-28 16:03:56</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.0622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1i9vydl</td>\n",
       "      <td>Warranty from Noon or Amazon</td>\n",
       "      <td>m9lj27y</td>\n",
       "      <td>Agitated-Fox2818</td>\n",
       "      <td>You are looking at a tag in noon app saying 1 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-28 10:38:59</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1i9vydl</td>\n",
       "      <td>Warranty from Noon or Amazon</td>\n",
       "      <td>m9lhuq1</td>\n",
       "      <td>m2bop</td>\n",
       "      <td>Are you sure? There's nothing indicating that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-28 10:28:09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                     post_title comment_id  \\\n",
       "0  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mjkdv   \n",
       "1  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mgv03   \n",
       "2  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mfqls   \n",
       "3  1i9vydl                   Warranty from Noon or Amazon    m9lj27y   \n",
       "4  1i9vydl                   Warranty from Noon or Amazon    m9lhuq1   \n",
       "\n",
       "             author                                       comment_text  score  \\\n",
       "0    99DragonMaster                          will reach border by noon      2   \n",
       "1   AgileBadger5988  Be careful of blowing dust during late morning...      9   \n",
       "2     jamesdongdong  Apart from boring things, you can managed. How...      1   \n",
       "3  Agitated-Fox2818  You are looking at a tag in noon app saying 1 ...      2   \n",
       "4             m2bop  Are you sure? There's nothing indicating that ...      1   \n",
       "\n",
       "           created_utc  is_submitter  contains_noon_mention     type  \\\n",
       "0  2025-01-28 16:33:14          True                   True  comment   \n",
       "1  2025-01-28 16:12:52         False                   True  comment   \n",
       "2  2025-01-28 16:03:56         False                   True  comment   \n",
       "3  2025-01-28 10:38:59         False                   True  comment   \n",
       "4  2025-01-28 10:28:09          True                   True  comment   \n",
       "\n",
       "          subreddit vader_label  vader_polarity  \n",
       "0  DubaiPetrolHeads     Neutral          0.0258  \n",
       "1  DubaiPetrolHeads    Negative         -0.5574  \n",
       "2  DubaiPetrolHeads    Positive          0.0622  \n",
       "3               NaN    Positive          0.1027  \n",
       "4               NaN    Positive          0.3182  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to perform sentiment analysis using VADER\n",
    "def analyze_sentiment(comment):\n",
    "    # Get the sentiment scores for the comment\n",
    "    sentiment_scores = analyzer.polarity_scores(comment)\n",
    "    \n",
    "    # Determine the sentiment label based on the compound score\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    # Return both the sentiment label and the compound polarity score\n",
    "    return sentiment, sentiment_scores['compound']\n",
    "\n",
    "labelled_commentList_df = commentList_df.copy()\n",
    "# Apply sentiment analysis to the 'textDisplay' column and create new columns for sentiment and polarity score\n",
    "labelled_commentList_df[['vader_label', 'vader_polarity']] = labelled_commentList_df['comment_text'].apply(lambda text: pd.Series(analyze_sentiment(text)))\n",
    "\n",
    "labelled_commentList_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Roberta-base-go-emotions(Pre-trained-model)\n",
    "\n",
    "The details of the pre-trained Roberta-base-go-emotions model can be viewed here: <a href=\"https://huggingface.co/SamLowe/roberta-base-go_emotions\">Roberta Pre-Trained Model </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip uninstall -y torch typing-extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install typing-extensions==4.7.1 torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'CUDA' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Test the classifier\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=\"SamLowe/roberta-base-go_emotions\",\n",
    "    top_k=None,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>contains_noon_mention</th>\n",
       "      <th>type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>textBlob_polarity</th>\n",
       "      <th>textBlob_label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>roberta_emotion_scores</th>\n",
       "      <th>roberta_positive_score</th>\n",
       "      <th>roberta_negative_score</th>\n",
       "      <th>roberta_neutral_score</th>\n",
       "      <th>roberta_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mjkdv</td>\n",
       "      <td>99DragonMaster</td>\n",
       "      <td>will reach border by noon</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-28 16:33:14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>reach border noon</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.9491845369338...</td>\n",
       "      <td>0.064729</td>\n",
       "      <td>0.014750</td>\n",
       "      <td>0.961027</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mgv03</td>\n",
       "      <td>AgileBadger5988</td>\n",
       "      <td>Be careful of blowing dust during late morning...</td>\n",
       "      <td>9</td>\n",
       "      <td>2025-01-28 16:12:52</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "      <td>-0.042857</td>\n",
       "      <td>negative</td>\n",
       "      <td>care blow dust late morn noon sand heat rout a...</td>\n",
       "      <td>[{'label': 'caring', 'score': 0.81811195611953...</td>\n",
       "      <td>0.155917</td>\n",
       "      <td>0.157256</td>\n",
       "      <td>0.128665</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ibzgq5</td>\n",
       "      <td>Dubai to Riyadh - Which Road to Take? 1 or 2?</td>\n",
       "      <td>m9mfqls</td>\n",
       "      <td>jamesdongdong</td>\n",
       "      <td>Apart from boring things, you can managed. How...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-28 16:03:56</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>DubaiPetrolHeads</td>\n",
       "      <td>-0.233333</td>\n",
       "      <td>negative</td>\n",
       "      <td>apart bore thing manag howev light pole road b...</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.5306403636932...</td>\n",
       "      <td>0.549081</td>\n",
       "      <td>0.029948</td>\n",
       "      <td>0.568553</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1i9vydl</td>\n",
       "      <td>Warranty from Noon or Amazon</td>\n",
       "      <td>m9lj27y</td>\n",
       "      <td>Agitated-Fox2818</td>\n",
       "      <td>You are looking at a tag in noon app saying 1 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-28 10:38:59</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>positive</td>\n",
       "      <td>look tag noon app say year warranti say noon g...</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.8981201052665...</td>\n",
       "      <td>0.125795</td>\n",
       "      <td>0.025358</td>\n",
       "      <td>0.923674</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1i9vydl</td>\n",
       "      <td>Warranty from Noon or Amazon</td>\n",
       "      <td>m9lhuq1</td>\n",
       "      <td>m2bop</td>\n",
       "      <td>Are you sure? There's nothing indicating that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-28 10:28:09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>positive</td>\n",
       "      <td>sure there noth indic read noon warranti polic...</td>\n",
       "      <td>[{'label': 'neutral', 'score': 0.5903054475784...</td>\n",
       "      <td>0.157819</td>\n",
       "      <td>0.192896</td>\n",
       "      <td>0.946390</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                     post_title comment_id  \\\n",
       "0  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mjkdv   \n",
       "1  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mgv03   \n",
       "2  1ibzgq5  Dubai to Riyadh - Which Road to Take? 1 or 2?    m9mfqls   \n",
       "3  1i9vydl                   Warranty from Noon or Amazon    m9lj27y   \n",
       "4  1i9vydl                   Warranty from Noon or Amazon    m9lhuq1   \n",
       "\n",
       "             author                                       comment_text  score  \\\n",
       "0    99DragonMaster                          will reach border by noon      2   \n",
       "1   AgileBadger5988  Be careful of blowing dust during late morning...      9   \n",
       "2     jamesdongdong  Apart from boring things, you can managed. How...      1   \n",
       "3  Agitated-Fox2818  You are looking at a tag in noon app saying 1 ...      2   \n",
       "4             m2bop  Are you sure? There's nothing indicating that ...      1   \n",
       "\n",
       "           created_utc  is_submitter  contains_noon_mention     type  \\\n",
       "0  2025-01-28 16:33:14          True                   True  comment   \n",
       "1  2025-01-28 16:12:52         False                   True  comment   \n",
       "2  2025-01-28 16:03:56         False                   True  comment   \n",
       "3  2025-01-28 10:38:59         False                   True  comment   \n",
       "4  2025-01-28 10:28:09          True                   True  comment   \n",
       "\n",
       "          subreddit  textBlob_polarity textBlob_label  \\\n",
       "0  DubaiPetrolHeads           0.000000        neutral   \n",
       "1  DubaiPetrolHeads          -0.042857       negative   \n",
       "2  DubaiPetrolHeads          -0.233333       negative   \n",
       "3               NaN           0.500000       positive   \n",
       "4               NaN           0.500000       positive   \n",
       "\n",
       "                                   preprocessed_text  \\\n",
       "0                                  reach border noon   \n",
       "1  care blow dust late morn noon sand heat rout a...   \n",
       "2  apart bore thing manag howev light pole road b...   \n",
       "3  look tag noon app say year warranti say noon g...   \n",
       "4  sure there noth indic read noon warranti polic...   \n",
       "\n",
       "                              roberta_emotion_scores  roberta_positive_score  \\\n",
       "0  [{'label': 'neutral', 'score': 0.9491845369338...                0.064729   \n",
       "1  [{'label': 'caring', 'score': 0.81811195611953...                0.155917   \n",
       "2  [{'label': 'neutral', 'score': 0.5306403636932...                0.549081   \n",
       "3  [{'label': 'neutral', 'score': 0.8981201052665...                0.125795   \n",
       "4  [{'label': 'neutral', 'score': 0.5903054475784...                0.157819   \n",
       "\n",
       "   roberta_negative_score  roberta_neutral_score roberta_label  \n",
       "0                0.014750               0.961027       Neutral  \n",
       "1                0.157256               0.128665       Neutral  \n",
       "2                0.029948               0.568553      Positive  \n",
       "3                0.025358               0.923674      Positive  \n",
       "4                0.192896               0.946390       Neutral  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import pipeline\n",
    "\n",
    "# Removed emotions:\n",
    "# \"caring\"\n",
    "\n",
    "emotions = {\n",
    "    \"positive\": [\"desire\",\"approval\", \"admiration\", \"gratitude\", \"optimism\", \"love\", \"relief\", \"joy\", \"pride\", \"excitement\", \"amusement\", \"curiosity\"],\n",
    "    \"negative\": [\"disapproval\", \"disappointment\", \"annoyance\", \"sadness\", \"anger\", \"disgust\", \"embarrassment\", \"remorse\", \"grief\", \"fear\", \"nervousness\"],\n",
    "    \"neutral\": [\"realization\", \"surprise\", \"neutral\", \"confusion\"]\n",
    "}\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
    "# List to store the sentiment analysis results\n",
    "sentiment_results = []\n",
    "comments = labelled_commentList_df['comment_text']\n",
    "threshold = 0.1\n",
    "truncate_count = 0;\n",
    "max_seq_length = 512\n",
    "\n",
    "# Define a function to truncate the input sequence\n",
    "def truncate_sequence(sequence, max_length):\n",
    "    global truncate_count\n",
    "    if len(sequence) > max_length:\n",
    "        truncate_count = truncate_count +1;\n",
    "        return sequence[:max_length]\n",
    "    else:\n",
    "        return sequence\n",
    "\n",
    "def get_score(emotion_label, emotion_array):\n",
    "    for emotion_obj in emotion_array:\n",
    "        if emotion_obj['label'] == emotion_label:\n",
    "            return emotion_obj['score']\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Iterate over the comments and analyze sentiment\n",
    "for comment in comments:\n",
    "    truncated_comment = truncate_sequence(comment, max_seq_length)\n",
    "    # Analyze sentiment for the truncated comment\n",
    "    emotion_scores = classifier(truncated_comment)\n",
    "    positive_score = sum(emotion['score'] for emotion in emotion_scores[0] if emotion['label'] in emotions[\"positive\"])\n",
    "    negative_score = sum(emotion['score'] for emotion in emotion_scores[0] if emotion['label'] in emotions[\"negative\"])\n",
    "    neutral_score = sum(emotion['score'] for emotion in emotion_scores[0] if emotion['label'] in emotions[\"neutral\"])\n",
    "    overall_polarity = \"Neutral\"\n",
    "    if positive_score - negative_score > threshold:\n",
    "        overall_polarity = \"Positive\"\n",
    "    elif negative_score - positive_score > threshold:\n",
    "        overall_polarity = \"Negative\"\n",
    "    sentiment_results.append({\n",
    "        'comment': comment,\n",
    "    'roberta_emotion_scores': emotion_scores[0],\n",
    "    'roberta_positive_score': positive_score,\n",
    "    'roberta_negative_score': negative_score,\n",
    "    'roberta_neutral_score': neutral_score,\n",
    "    'roberta_label': overall_polarity})\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_results)\n",
    "\n",
    "# Drop the 'comment' column from sentiment_df\n",
    "sentiment_df.drop(columns=['comment'], inplace=True)\n",
    "\n",
    "# Merge the sentiment results DataFrame with the labelled_commentList_df DataFrame\n",
    "labelled_commentList_df = pd.concat([labelled_commentList_df, sentiment_df], axis=1)\n",
    "labelled_commentList_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TextBlob (sentiment lexicon):\n",
    "\n",
    "Text Blob’s sentiment analysis algorithm computes text sentiment by aggregating polarity scores assigned to individual words and phrases, with values ranging from -1 to +1, representing negativity, positivity, and neutrality respectively. This process yields an overall polarity score, along with a subjectivity score indicating the text's subjective nature. Text Blob provides part-of-speech tagging functionality which enhances its contextual comprehension by identifying the grammatical structure and syntactic relationships within a text. By analysing the parts of speech in a sentence, Text Blob can gain limited insight into the context in which words are used, unlike Vader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\pratibha\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\pratibha\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\pratibha\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pratibha\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pratibha\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pratibha\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pratibha\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         comment_text  textBlob_polarity  \\\n",
      "0                           will reach border by noon           0.000000   \n",
      "1   Be careful of blowing dust during late morning...          -0.042857   \n",
      "2   Apart from boring things, you can managed. How...          -0.233333   \n",
      "3   You are looking at a tag in noon app saying 1 ...           0.500000   \n",
      "4   Are you sure? There's nothing indicating that ...           0.500000   \n",
      "5   Warranty is given by OnePlus. Not Noon or Amaz...           0.000000   \n",
      "6   I am aware of this, but Noon still offers a 12...           0.240000   \n",
      "7   If you order from Noon, there is a chance you ...           0.300000   \n",
      "8   It’s not Arabic it’s transliterated English: w...           0.000000   \n",
      "9   Plot twist - noon did those mistakes on purpos...           0.000000   \n",
      "10  This perfume shop guy told me it is 250 aed . ...          -0.250000   \n",
      "11                          So noon must be og right?           0.285714   \n",
      "12  The one on noon is sold by ‘noon’ themselves. ...           0.115000   \n",
      "13                                 noon order inside?           0.000000   \n",
      "14  I wouldn’t suggest buying anything from noon, ...           0.052083   \n",
      "15  I have similar experience and noon is refusing...           0.100000   \n",
      "16  I bought ps5 slim disc UAE version from Noon f...           0.566667   \n",
      "17  On the food part try using talabat or noon or ...           0.000000   \n",
      "18  I got my switch 1, switch OLED and ps5 i gifte...           0.333333   \n",
      "19  What the guy said Just spend 100 200 more and ...           0.260000   \n",
      "\n",
      "   textBlob_label  \n",
      "0         neutral  \n",
      "1        negative  \n",
      "2        negative  \n",
      "3        positive  \n",
      "4        positive  \n",
      "5         neutral  \n",
      "6        positive  \n",
      "7        positive  \n",
      "8         neutral  \n",
      "9         neutral  \n",
      "10       negative  \n",
      "11       positive  \n",
      "12       positive  \n",
      "13        neutral  \n",
      "14       positive  \n",
      "15       positive  \n",
      "16       positive  \n",
      "17        neutral  \n",
      "18       positive  \n",
      "19       positive  \n",
      "\n",
      "Distribution of sentiments:\n",
      "textBlob_label\n",
      "positive    1865\n",
      "neutral      922\n",
      "negative     811\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_comment_polarity(comment):\n",
    "    comment_text = str(comment) \n",
    "    analysis = TextBlob(comment_text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    return polarity\n",
    "\n",
    "def get_polarity_label(polarity):\n",
    "    label = ''\n",
    "    if polarity == 0:\n",
    "        label = 'neutral'\n",
    "    elif polarity < 0.00:\n",
    "        label = 'negative'\n",
    "    elif polarity > 0.00:\n",
    "        label = 'positive'\n",
    "    return label\n",
    "            \n",
    "labelled_commentList_df = commentList_df.copy()\n",
    "\n",
    "\n",
    "# First, calculate polarity scores\n",
    "labelled_commentList_df['textBlob_polarity'] = labelled_commentList_df['comment_text'].apply(get_comment_polarity)\n",
    "\n",
    "# Then, use these polarity scores to get labels\n",
    "labelled_commentList_df['textBlob_label'] = labelled_commentList_df['textBlob_polarity'].apply(get_polarity_label)\n",
    "\n",
    "# Display the DataFrame with the added textBlob columns\n",
    "print(labelled_commentList_df[['comment_text', 'textBlob_polarity', 'textBlob_label']].head(20))\n",
    "\n",
    "# Show distribution of sentiments\n",
    "print(\"\\nDistribution of sentiments:\")\n",
    "print(labelled_commentList_df['textBlob_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the sentiment analysed data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to sentiment_analyzed_data.csv\n"
     ]
    }
   ],
   "source": [
    "labelled_commentList_df.to_csv('sentiment_analyzed_data.csv', index=False)\n",
    "print(\"Data saved successfully to sentiment_analyzed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the sentiment analysed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labelled_commentList_df = pd.read_csv(\"./sentiment_analyzed_data.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset:\n",
    "\n",
    "Splitting the entire dataset into 80% as training and evaluation dataset, and the rest 20% as test dataset for training and testing purpose of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2878\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = labelled_commentList_df['comment_text'] \n",
    "y = labelled_commentList_df['textBlob_label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Error downloading 'vader_lexicon' from\n",
      "[nltk_data]    |     <https://raw.githubusercontent.com/nltk/nltk_data\n",
      "[nltk_data]    |     /gh-pages/packages/sentiment/vader_lexicon.zip>:\n",
      "[nltk_data]    |     <urlopen error [WinError 10065] A socket\n",
      "[nltk_data]    |     operation was attempted to an unreachable host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pratibha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Preprocessed text:\n",
      "                                        comment_text  \\\n",
      "0                          will reach border by noon   \n",
      "1  Be careful of blowing dust during late morning...   \n",
      "2  Apart from boring things, you can managed. How...   \n",
      "3  You are looking at a tag in noon app saying 1 ...   \n",
      "4  Are you sure? There's nothing indicating that ...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0                                  reach border noon  \n",
      "1  care blow dust late morn noon sand heat rout a...  \n",
      "2  apart bore thing manag howev light pole road b...  \n",
      "3  look tag noon app say year warranti say noon g...  \n",
      "4  sure there noth indic read noon warranti polic...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "   text = text.lower()\n",
    "   text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "   tokens = word_tokenize(text)\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "   tokens = [token for token in tokens if token not in stop_words]\n",
    "   stemmer = PorterStemmer()\n",
    "   tokens = [stemmer.stem(token) for token in tokens]\n",
    "   return ' '.join(tokens)\n",
    "\n",
    "labelled_commentList_df['preprocessed_text'] = labelled_commentList_df['comment_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Original vs Preprocessed text:\")\n",
    "print(labelled_commentList_df[['comment_text', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Representation - Binary Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of binary vector representation: (3598, 5792)\n",
      "\n",
      "First few rows and columns of binary representation:\n",
      "   a  aa  aaaand  aah  abaar  abbar  abid  abil  abl  abomin\n",
      "0  0   0       0    0      0      0     0     0    0       0\n",
      "1  0   0       0    0      0      0     0     0    0       0\n",
      "2  0   0       0    0      0      0     0     0    0       0\n",
      "3  0   0       0    0      0      0     0     0    0       0\n",
      "4  0   0       0    0      0      0     0     0    0       0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer for binary representation\n",
    "binary_vectorizer = CountVectorizer(binary=True, token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "# Fit and transform using the preprocessed_text column\n",
    "X_binary = binary_vectorizer.fit_transform(labelled_commentList_df['preprocessed_text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = binary_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame with binary vectors\n",
    "binary_df = pd.DataFrame(X_binary.toarray(), columns=feature_names)\n",
    "\n",
    "# Save to CSV\n",
    "binary_df.to_csv('vector_binary.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Print shape and first few rows\n",
    "print(\"Shape of binary vector representation:\", binary_df.shape)\n",
    "print(\"\\nFirst few rows and columns of binary representation:\")\n",
    "print(binary_df.iloc[:5, :10])  # Show first 5 rows and 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Representation - Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      a  aa  aaaand  aah  abaar  abbar  abid  abil  abl  abomin  ...  zero  \\\n",
      "0     0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "1     0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "2     0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "3     0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "4     0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "...  ..  ..     ...  ...    ...    ...   ...   ...  ...     ...  ...   ...   \n",
      "3593  0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "3594  0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "3595  0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "3596  0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "3597  0   0       0    0      0      0     0     0    0       0  ...     0   \n",
      "\n",
      "      zeroohm  zillion  zip  ziplin  zomato  zomatotalabatcareem  zone  zoom  \\\n",
      "0           0        0    0       0       0                    0     0     0   \n",
      "1           0        0    0       0       0                    0     0     0   \n",
      "2           0        0    0       0       0                    0     0     0   \n",
      "3           0        0    0       0       0                    0     0     0   \n",
      "4           0        0    0       0       0                    0     0     0   \n",
      "...       ...      ...  ...     ...     ...                  ...   ...   ...   \n",
      "3593        0        0    0       0       0                    0     0     0   \n",
      "3594        0        0    0       0       0                    0     0     0   \n",
      "3595        0        0    0       0       0                    0     0     0   \n",
      "3596        0        0    0       0       0                    0     0     0   \n",
      "3597        0        0    0       0       0                    0     0     0   \n",
      "\n",
      "      zoomonlin  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "...         ...  \n",
      "3593          0  \n",
      "3594          0  \n",
      "3595          0  \n",
      "3596          0  \n",
      "3597          0  \n",
      "\n",
      "[3598 rows x 5792 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "# Fit and transform using the preprocessed_text column\n",
    "X = vectorizer.fit_transform(labelled_commentList_df['preprocessed_text'])\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to display the frequency counts\n",
    "count_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "# Save the file\n",
    "count_df.to_csv('frequency_count.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Representation - TF-IDF(Unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        a   aa  aaaand  aah  abaar  abbar  abid  abil  abl  abomin  ...  zero  \\\n",
      "0     0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "1     0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "2     0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "3     0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "4     0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "...   ...  ...     ...  ...    ...    ...   ...   ...  ...     ...  ...   ...   \n",
      "3593  0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "3594  0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "3595  0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "3596  0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "3597  0.0  0.0     0.0  0.0    0.0    0.0   0.0   0.0  0.0     0.0  ...   0.0   \n",
      "\n",
      "      zeroohm  zillion  zip  ziplin  zomato  zomatotalabatcareem  zone  zoom  \\\n",
      "0         0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "1         0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "2         0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "3         0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "4         0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "...       ...      ...  ...     ...     ...                  ...   ...   ...   \n",
      "3593      0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "3594      0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "3595      0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "3596      0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "3597      0.0      0.0  0.0     0.0     0.0                  0.0   0.0   0.0   \n",
      "\n",
      "      zoomonlin  \n",
      "0           0.0  \n",
      "1           0.0  \n",
      "2           0.0  \n",
      "3           0.0  \n",
      "4           0.0  \n",
      "...         ...  \n",
      "3593        0.0  \n",
      "3594        0.0  \n",
      "3595        0.0  \n",
      "3596        0.0  \n",
      "3597        0.0  \n",
      "\n",
      "[3598 rows x 5792 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "# Fit and transform using the preprocessed_text column\n",
    "X_tfidf = vectorizer.fit_transform(labelled_commentList_df['preprocessed_text'])\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to display the TF-IDF values\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=feature_names_tfidf)\n",
    "\n",
    "# Save the file\n",
    "tfidf_df.to_csv('tfidf.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Space Representation TF-IDF(Bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      a a  a addit  a almost  a also  a alway  a amazon  \\\n",
      "0     0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "1     0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "2     0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "3     0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "4     0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "...   ...      ...       ...     ...      ...       ...   \n",
      "3593  0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "3594  0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "3595  0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "3596  0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "3597  0.0      0.0       0.0     0.0      0.0       0.0   \n",
      "\n",
      "      a amazonaehttpsamazona  a amazoncom  a amazonnoon  a anoth  ...  \\\n",
      "0                        0.0          0.0           0.0      0.0  ...   \n",
      "1                        0.0          0.0           0.0      0.0  ...   \n",
      "2                        0.0          0.0           0.0      0.0  ...   \n",
      "3                        0.0          0.0           0.0      0.0  ...   \n",
      "4                        0.0          0.0           0.0      0.0  ...   \n",
      "...                      ...          ...           ...      ...  ...   \n",
      "3593                     0.0          0.0           0.0      0.0  ...   \n",
      "3594                     0.0          0.0           0.0      0.0  ...   \n",
      "3595                     0.0          0.0           0.0      0.0  ...   \n",
      "3596                     0.0          0.0           0.0      0.0  ...   \n",
      "3597                     0.0          0.0           0.0      0.0  ...   \n",
      "\n",
      "      zomato talabat  zomato thstreet  zomato travel  zomato work  \\\n",
      "0                0.0              0.0            0.0          0.0   \n",
      "1                0.0              0.0            0.0          0.0   \n",
      "2                0.0              0.0            0.0          0.0   \n",
      "3                0.0              0.0            0.0          0.0   \n",
      "4                0.0              0.0            0.0          0.0   \n",
      "...              ...              ...            ...          ...   \n",
      "3593             0.0              0.0            0.0          0.0   \n",
      "3594             0.0              0.0            0.0          0.0   \n",
      "3595             0.0              0.0            0.0          0.0   \n",
      "3596             0.0              0.0            0.0          0.0   \n",
      "3597             0.0              0.0            0.0          0.0   \n",
      "\n",
      "      zomatotalabatcareem forese  zone compani  zone see  zoom carrefour  \\\n",
      "0                            0.0           0.0       0.0             0.0   \n",
      "1                            0.0           0.0       0.0             0.0   \n",
      "2                            0.0           0.0       0.0             0.0   \n",
      "3                            0.0           0.0       0.0             0.0   \n",
      "4                            0.0           0.0       0.0             0.0   \n",
      "...                          ...           ...       ...             ...   \n",
      "3593                         0.0           0.0       0.0             0.0   \n",
      "3594                         0.0           0.0       0.0             0.0   \n",
      "3595                         0.0           0.0       0.0             0.0   \n",
      "3596                         0.0           0.0       0.0             0.0   \n",
      "3597                         0.0           0.0       0.0             0.0   \n",
      "\n",
      "      zoom goto  zoomonlin good  \n",
      "0           0.0             0.0  \n",
      "1           0.0             0.0  \n",
      "2           0.0             0.0  \n",
      "3           0.0             0.0  \n",
      "4           0.0             0.0  \n",
      "...         ...             ...  \n",
      "3593        0.0             0.0  \n",
      "3594        0.0             0.0  \n",
      "3595        0.0             0.0  \n",
      "3596        0.0             0.0  \n",
      "3597        0.0             0.0  \n",
      "\n",
      "[3598 rows x 50494 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer for bigram features\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z]+\\b', ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform using the preprocessed_text column\n",
    "X_tfidf = vectorizer.fit_transform(labelled_commentList_df['preprocessed_text'])\n",
    "\n",
    "# Get the feature names (bigrams)\n",
    "feature_names_tfidf = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to display the TF-IDF values\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=feature_names_tfidf)\n",
    "\n",
    "# Save the file\n",
    "tfidf_df.to_csv('tfidf_bigram.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline:\n",
    "\n",
    "To build an efficient text analytic pipeline, the pre-processed comments are comments are represented by 3 different vector representation methods, which are as below:\n",
    "<br><b>1.\tUsing both countVectorizer and tfidfVectorizer</b>\n",
    "<br><b>2.\tUsing only countVectorizer</b>\n",
    "<br><b>3.\tUsing only tfidfVectorizer</b>\n",
    "\n",
    "These vector space represented text are then passed through 3 different classifiers to compare and analyse the models. The classifiers are as below:\n",
    "<br><b>1.\tNaïve bayes</b>\n",
    "<br><b>2.\tSupport Vector Machine (SVM)</b>\n",
    "<br><b>3.\tLogistic Regression</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using both countVectorizer and tfidf(TextBlob):\n",
    "\n",
    "In the pipeline using both countVectorizer and tfidfVectorizer, different parameters were varied like binary and max_df for countVectorizer, ngram_range and max_df for tfidfVectorizer. The max_df for both vectorizers were 0.20, 0.50, and 1.0. On tuning the hyperparameters, GridSerachCV is performed to find the best parameters and score from these combinations for each of the classifiers are displayed. Classification report and confusion matrix are also displayed for the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Logistic Regression\n",
      "Executing the pipeline....\n",
      "Best parameters: {'features__frequency_count__binary': True, 'features__frequency_count__max_df': 0.2, 'features__tfidf_vectorizer__max_df': 1.0, 'features__tfidf_vectorizer__ngram_range': (1, 3)}\n",
      "Best score: 0.7536467391304348\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.54      0.57       154\n",
      "     neutral       0.73      0.82      0.78       200\n",
      "    positive       0.81      0.80      0.80       366\n",
      "\n",
      "    accuracy                           0.75       720\n",
      "   macro avg       0.72      0.72      0.72       720\n",
      "weighted avg       0.75      0.75      0.75       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 83  23  48]\n",
      " [ 14 165  21]\n",
      " [ 38  37 291]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: SVM\n",
      "Executing the pipeline....\n",
      "Best parameters: {'features__frequency_count__binary': True, 'features__frequency_count__max_df': 0.5, 'features__tfidf_vectorizer__max_df': 0.2, 'features__tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.6900609903381643\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.27      0.40       154\n",
      "     neutral       0.73      0.83      0.78       200\n",
      "    positive       0.73      0.87      0.80       366\n",
      "\n",
      "    accuracy                           0.73       720\n",
      "   macro avg       0.75      0.66      0.66       720\n",
      "weighted avg       0.74      0.73      0.71       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 42  22  90]\n",
      " [  6 167  27]\n",
      " [  6  41 319]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Executing the pipeline....\n",
      "Best parameters: {'features__frequency_count__binary': False, 'features__frequency_count__max_df': 0.2, 'features__tfidf_vectorizer__max_df': 1.0, 'features__tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.6160573671497585\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.27      0.36       154\n",
      "     neutral       0.83      0.22      0.35       200\n",
      "    positive       0.58      0.95      0.72       366\n",
      "\n",
      "    accuracy                           0.60       720\n",
      "   macro avg       0.66      0.48      0.48       720\n",
      "weighted avg       0.65      0.60      0.54       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 41   4 109]\n",
      " [ 16  44 140]\n",
      " [ 14   5 347]]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define parameters for CountVectorizer\n",
    "count_vectorizer_params = {\n",
    "    'binary': [True, False],\n",
    "    'max_df': [0.20, 0.50, 1.0]\n",
    "}\n",
    "\n",
    "# Define parameters for TfidfVectorizer\n",
    "tfidf_vectorizer_params = {\n",
    "    'ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigram, bigram, trigram\n",
    "    'max_df': [0.20, 0.50, 1.0]\n",
    "}\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Define features using FeatureUnion\n",
    "features = FeatureUnion([\n",
    "    ('frequency_count', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'features__frequency_count__binary': count_vectorizer_params['binary'],\n",
    "    'features__frequency_count__max_df': count_vectorizer_params['max_df'],\n",
    "    'features__tfidf_vectorizer__ngram_range': tfidf_vectorizer_params['ngram_range'],\n",
    "    'features__tfidf_vectorizer__max_df': tfidf_vectorizer_params['max_df']\n",
    "}\n",
    "\n",
    "# Use preprocessed text and Text Blob labels\n",
    "X = labelled_commentList_df['preprocessed_text']\n",
    "y = labelled_commentList_df['textBlob_label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loop through classifiers\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(f\"\\nClassifier: {classifier_name}\")\n",
    "    print(\"Executing the pipeline....\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('features', features),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    \n",
    "    # Get predictions\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using both countVectorizer and tfidf(Roberta):\n",
    "\n",
    "In the pipeline using both countVectorizer and tfidfVectorizer, different parameters were varied like binary and max_df for countVectorizer, ngram_range and max_df for tfidfVectorizer. The max_df for both vectorizers were 0.20, 0.50, and 1.0. On tuning the hyperparameters, GridSerachCV is performed to find the best parameters and score from these combinations for each of the classifiers are displayed. Classification report and confusion matrix are also displayed for the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Logistic Regression\n",
      "Executing the pipeline....\n",
      "Best parameters: {'features__frequency_count__binary': True, 'features__frequency_count__max_df': 0.2, 'features__tfidf_vectorizer__max_df': 0.5, 'features__tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.5920923913043479\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.56      0.48      0.52       193\n",
      "     Neutral       0.47      0.49      0.48       192\n",
      "    Positive       0.64      0.68      0.66       335\n",
      "\n",
      "    accuracy                           0.58       720\n",
      "   macro avg       0.56      0.55      0.55       720\n",
      "weighted avg       0.57      0.58      0.57       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 92  38  63]\n",
      " [ 31  94  67]\n",
      " [ 40  66 229]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: SVM\n",
      "Executing the pipeline....\n",
      "Best parameters: {'features__frequency_count__binary': True, 'features__frequency_count__max_df': 1.0, 'features__tfidf_vectorizer__max_df': 1.0, 'features__tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.5806141304347826\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.36      0.47       193\n",
      "     Neutral       0.56      0.27      0.36       192\n",
      "    Positive       0.55      0.86      0.67       335\n",
      "\n",
      "    accuracy                           0.57       720\n",
      "   macro avg       0.59      0.50      0.50       720\n",
      "weighted avg       0.58      0.57      0.53       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 70  11 112]\n",
      " [ 18  51 123]\n",
      " [ 18  29 288]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Executing the pipeline....\n",
      "Best parameters: {'features__frequency_count__binary': True, 'features__frequency_count__max_df': 1.0, 'features__tfidf_vectorizer__max_df': 1.0, 'features__tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.5795730676328502\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.56      0.41      0.47       193\n",
      "     Neutral       0.50      0.14      0.22       192\n",
      "    Positive       0.54      0.85      0.66       335\n",
      "\n",
      "    accuracy                           0.54       720\n",
      "   macro avg       0.53      0.47      0.45       720\n",
      "weighted avg       0.54      0.54      0.49       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 79   7 107]\n",
      " [ 31  27 134]\n",
      " [ 31  20 284]]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define parameters for CountVectorizer\n",
    "count_vectorizer_params = {\n",
    "    'binary': [True, False],\n",
    "    'max_df': [0.20, 0.50, 1.0]\n",
    "}\n",
    "\n",
    "# Define parameters for TfidfVectorizer\n",
    "tfidf_vectorizer_params = {\n",
    "    'ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigram, bigram, trigram\n",
    "    'max_df': [0.20, 0.50, 1.0]\n",
    "}\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Define features using FeatureUnion\n",
    "features = FeatureUnion([\n",
    "    ('frequency_count', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'features__frequency_count__binary': count_vectorizer_params['binary'],\n",
    "    'features__frequency_count__max_df': count_vectorizer_params['max_df'],\n",
    "    'features__tfidf_vectorizer__ngram_range': tfidf_vectorizer_params['ngram_range'],\n",
    "    'features__tfidf_vectorizer__max_df': tfidf_vectorizer_params['max_df']\n",
    "}\n",
    "\n",
    "# Use preprocessed text and Roberta labels\n",
    "X = labelled_commentList_df['preprocessed_text']\n",
    "y = labelled_commentList_df['roberta_label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loop through classifiers\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(f\"\\nClassifier: {classifier_name}\")\n",
    "    print(\"Executing the pipeline....\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('features', features),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    \n",
    "    # Get predictions\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob Label Distribution:\n",
      "textBlob_label\n",
      "positive    1865\n",
      "neutral      922\n",
      "negative     811\n",
      "Name: count, dtype: int64\n",
      "\n",
      "RoBERTa Label Distribution:\n",
      "roberta_label\n",
      "Positive    1702\n",
      "Negative     954\n",
      "Neutral      942\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"TextBlob Label Distribution:\")\n",
    "print(labelled_commentList_df['textBlob_label'].value_counts())\n",
    "\n",
    "print(\"\\nRoBERTa Label Distribution:\")\n",
    "print(labelled_commentList_df['roberta_label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using only countVectorizer:\n",
    "\n",
    "In this pipeline, only countVectorizer method is used and the max_df were 0.2, 0.5 and 1. The same steps from the 1st pipeline was followed to obtain the best models for the 3 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Logistic Regression\n",
      "Executing the pipeline....\n",
      "Best parameters: {'frequency_count__binary': True, 'frequency_count__max_df': 0.5}\n",
      "Best score: 0.7494788647342995\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.56      0.59       154\n",
      "     neutral       0.75      0.84      0.79       200\n",
      "    positive       0.82      0.80      0.81       366\n",
      "\n",
      "    accuracy                           0.76       720\n",
      "   macro avg       0.73      0.73      0.73       720\n",
      "weighted avg       0.76      0.76      0.76       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 87  20  47]\n",
      " [ 14 168  18]\n",
      " [ 38  36 292]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: SVM\n",
      "Executing the pipeline....\n",
      "Best parameters: {'frequency_count__binary': True, 'frequency_count__max_df': 0.2}\n",
      "Best score: 0.6879746376811595\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.27      0.40       154\n",
      "     neutral       0.72      0.83      0.77       200\n",
      "    positive       0.73      0.87      0.79       366\n",
      "\n",
      "    accuracy                           0.73       720\n",
      "   macro avg       0.74      0.66      0.66       720\n",
      "weighted avg       0.74      0.73      0.70       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 42  21  91]\n",
      " [  6 166  28]\n",
      " [  6  42 318]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Executing the pipeline....\n",
      "Best parameters: {'frequency_count__binary': True, 'frequency_count__max_df': 1.0}\n",
      "Best score: 0.6101491545893719\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.30      0.39       154\n",
      "     neutral       0.75      0.20      0.31       200\n",
      "    positive       0.57      0.92      0.71       366\n",
      "\n",
      "    accuracy                           0.58       720\n",
      "   macro avg       0.63      0.47      0.47       720\n",
      "weighted avg       0.62      0.58      0.53       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 46   5 103]\n",
      " [ 14  39 147]\n",
      " [ 22   8 336]]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define parameters for CountVectorizer\n",
    "count_vectorizer_params = {\n",
    "    'binary': [True, False],\n",
    "    'max_df': [0.20, 0.50, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'frequency_count__binary': count_vectorizer_params['binary'],\n",
    "    'frequency_count__max_df': count_vectorizer_params['max_df'],\n",
    "    }\n",
    "\n",
    "# Use preprocessed text and Text Blob labels\n",
    "X = labelled_commentList_df['preprocessed_text']\n",
    "y = labelled_commentList_df['textBlob_label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loop through classifiers\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(f\"\\nClassifier: {classifier_name}\")\n",
    "    print(\"Executing the pipeline....\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('frequency_count', CountVectorizer()),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    \n",
    "    # Get predictions\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using only tfidf:\n",
    "\n",
    "This pipeline used only tfidfVectorizer and different parameters were varied like ngram_range [ unigram, bigram, and trigram] and max_df [0.2,0.5, 1.0]. Similar to the other pipelines, these hyperparameters were tuned and GridSerachCV was performed to obtain the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Logistic Regression\n",
      "Executing the pipeline....\n",
      "Best parameters: {'tfidf_vectorizer__max_df': 0.2, 'tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.71681038647343\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.38      0.51       154\n",
      "     neutral       0.74      0.61      0.67       200\n",
      "    positive       0.69      0.90      0.79       366\n",
      "\n",
      "    accuracy                           0.71       720\n",
      "   macro avg       0.73      0.63      0.65       720\n",
      "weighted avg       0.72      0.71      0.69       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 59  17  78]\n",
      " [ 10 122  68]\n",
      " [  9  26 331]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: SVM\n",
      "Executing the pipeline....\n",
      "Best parameters: {'tfidf_vectorizer__max_df': 1.0, 'tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.6817185990338164\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.26      0.40       154\n",
      "     neutral       0.76      0.55      0.64       200\n",
      "    positive       0.65      0.95      0.77       366\n",
      "\n",
      "    accuracy                           0.69       720\n",
      "   macro avg       0.77      0.58      0.60       720\n",
      "weighted avg       0.74      0.69      0.66       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 40  16  98]\n",
      " [  3 109  88]\n",
      " [  1  18 347]]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Executing the pipeline....\n",
      "Best parameters: {'tfidf_vectorizer__max_df': 1.0, 'tfidf_vectorizer__ngram_range': (1, 1)}\n",
      "Best score: 0.5330066425120773\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.02      0.04       154\n",
      "     neutral       0.56      0.03      0.05       200\n",
      "    positive       0.51      0.99      0.68       366\n",
      "\n",
      "    accuracy                           0.52       720\n",
      "   macro avg       0.69      0.35      0.25       720\n",
      "weighted avg       0.63      0.52      0.37       720\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  3   1 150]\n",
      " [  0   5 195]\n",
      " [  0   3 363]]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define parameters for TfidfVectorizer\n",
    "tfidf_vectorizer_params = {\n",
    "    'ngram_range': [(1, 1), (1, 2), (1, 3)],  # Unigram, bigram, trigram\n",
    "    'max_df': [0.20, 0.50, 1.0]\n",
    "}\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'tfidf_vectorizer__ngram_range': tfidf_vectorizer_params['ngram_range'],\n",
    "    'tfidf_vectorizer__max_df': tfidf_vectorizer_params['max_df']\n",
    "}\n",
    "\n",
    "# Use preprocessed text and Text Blob labels\n",
    "X = labelled_commentList_df['preprocessed_text']\n",
    "y = labelled_commentList_df['textBlob_label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loop through classifiers\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(f\"\\nClassifier: {classifier_name}\")\n",
    "    print(\"Executing the pipeline....\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best score:\", grid_search.best_score_)\n",
    "    \n",
    "    # Get predictions\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
